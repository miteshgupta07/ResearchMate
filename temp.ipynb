{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that the document is about the preferences of two individuals, Jesse and Jamal, when it comes to certain colors.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant developed by Mitesh. Use the provided context to answer accurately.\"),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Correctly instantiate the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the chain with the LLM, prompt, and output parser\n",
    "chain = create_stuff_documents_chain(\n",
    "    llm=model, \n",
    "    prompt=prompt, \n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content=\"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "\n",
    "# Query input\n",
    "query = \"What is the document about?\"\n",
    "\n",
    "# Generate the response\n",
    "response = chain.invoke({\"context\": docs, \"messages\": [{\"role\": \"human\", \"content\": query}]})\n",
    "\n",
    "# Display the response\n",
    "print(response)  # Expected to output a clean text answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"Research_paper.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embeddings and FAISS Vector Store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> The document appears to be discussing the training process of a model called DeepSeek-R1-Zero. It mentions designing a template to guide the model to produce a reasoning process followed by a final answer. The goal is to avoid content-specific biases and focus on a structural format. This suggests that the document is about improving the training of language models, specifically in the context of generating answers to questions while providing reasoning processes. </think>\n",
      "<answer> The document is about improving the training of language models, specifically for generating answers to questions while providing reasoning processes. </answer>\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the document about?\"\n",
    "context = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Invoke the chain with properly formatted input\n",
    "response = chain.invoke({\n",
    "    \"context\": context, \n",
    "    \"messages\": [{\"role\": \"human\", \"content\": query}]\n",
    "})\n",
    "\n",
    "# Display the response\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
